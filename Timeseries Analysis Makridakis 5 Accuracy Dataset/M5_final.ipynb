{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NikcZTp9ARaC",
        "y2cumUb3DWhS",
        "yn4m6NZYvEW8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmLNuH5nktTT"
      },
      "source": [
        "##**Installations**\n",
        "\n",
        "- Installing **darts**\n",
        "- Installing **Optuna**\n",
        "- Installing **Plotly**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9jihBkhmehT"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "packages = [\"darts\", \"optuna\", \"plotly\",\"LightGBM\"]\n",
        "\n",
        "for package in packages:\n",
        "    try:\n",
        "        __import__(package)\n",
        "        print(f\"{package} is installed\")\n",
        "    except ImportError:\n",
        "        print(f\"{package} is not installed. Installing now...\")\n",
        "        install(package)\n",
        "\n",
        "print(\"Success\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy6u-FWr_iGC"
      },
      "source": [
        "##**Imports**\n",
        "\n",
        "In this cell, we import all the necessary modules for our analysis.\n",
        "\n",
        "- **Numpy and Pandas:** These are fundamental packages for scientific computing and data manipulation in Python.\n",
        "- **Matplotlib:** This is a plotting library that we'll use for data visualization.\n",
        "- **Sklearn:** We'll use this library for data preprocessing and performance metrics.\n",
        "- **Optuna:** This is a hyperparameter optimization framework, which we'll use to tune our N-BEATS model.\n",
        "- **Darts:** This library provides us with utilities for time series processing and models, including the N-BEATS model.\n",
        "- **Google Colab:** We use Google Colab's drive module to mount our Google Drive where our data is stored.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyThSeMDTYjZ"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from typing import Union, List\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from IPython.display import display\n",
        "import time\n",
        "import gc\n",
        "#import seaborn as sns\n",
        "\n",
        "#plotly\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "\n",
        "# optuna hyperparameter\n",
        "import optuna\n",
        "from optuna.integration import PyTorchLightningPruningCallback\n",
        "import optuna.visualization as ovis\n",
        "\n",
        "#darts imports\n",
        "from darts import TimeSeries\n",
        "from darts.models import NaiveSeasonal, NaiveDrift, ExponentialSmoothing, VARIMA, AutoARIMA\n",
        "from darts.models import BlockRNNModel, TFTModel, NBEATSModel, NLinearModel, DLinearModel, LightGBMModel, TCNModel, LinearRegressionModel, RandomForest\n",
        "from darts.dataprocessing.transformers import Scaler, MissingValuesFiller, InvertibleMapper\n",
        "from darts.metrics import mape, smape, mae, r2_score, mse, smape, mase,rmse, rmsle\n",
        "from darts.utils.timeseries_generation import (gaussian_timeseries,linear_timeseries,sine_timeseries)\n",
        "from darts.dataprocessing import Pipeline\n",
        "\n",
        "#Validation Imports\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
        "from sklearn.utils.validation import _deprecate_positional_args\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Drive mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')     #Drive abcr5914\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NikcZTp9ARaC"
      },
      "source": [
        "---\n",
        "## **Required Functions**\n",
        "\n",
        "The following code blocks contain a collection of helper functions that are essential for various tasks in the notebook, such as data preprocessing, model training, and evaluation. Each function is briefly described below:\n",
        "\n",
        "### 1. **fit_predict_and_evaluate**\n",
        "This function is responsible for training a given model on a dataset, making predictions, and evaluating its performance. It uses metrics such as Mean Absolute Error (MAE), Symmetric Mean Absolute Percentage Error (sMAPE), and Root Mean Square Error (RMSE). This comprehensive function encapsulates the process of fitting, predicting, and evaluating a model.\n",
        "```python\n",
        "def fit_predict_and_evaluate(model, training_set, validation_set, n_forecasts, past_covariates_train=None, past_covariates_validation=None, series_list=False, model_name='Model'):\n",
        "```\n",
        "\n",
        "### 2. **extend_index_with_past**\n",
        "This function is designed to extend a given index in a list with its preceding indexes based on a specified chunk length. It is primarily used in time series analysis where context from past data points is crucial.\n",
        "```python\n",
        "def extend_index_with_past(input_chunk_length, train_idx, test_idx, dataframe, gap=False):\n",
        "```\n",
        "\n",
        "### 3. **add_metrics_to_df**\n",
        "This function adds various performance metrics of a model to a DataFrame. It's useful for tracking and comparing the performance of different models across various parameters.\n",
        "```python\n",
        "def add_metrics_to_df(df, store_id, dept_id, model_name, mae, smape, rmse, training_time, forecast_time, total_time, dataset_size):\n",
        "```\n",
        "\n",
        "### 4. **load_dataset**\n",
        "A simple utility function for loading a dataset from a specified path. This function is particularly useful when working with CSV files.\n",
        "```python\n",
        "def load_dataset(path):\n",
        "```\n",
        "\n",
        "### 5. **one_hot_encode_and_join**\n",
        "This function performs one-hot encoding on a specified feature of a DataFrame and then joins the encoded data back to the original DataFrame. It's an essential step in preprocessing categorical variables for machine learning models.\n",
        "```python\n",
        "def one_hot_encode_and_join(original_dataframe, feature_to_encode, new_name=None):\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BC1Nifl70XK1"
      },
      "outputs": [],
      "source": [
        "def fit_predict_and_evaluate(model, training_set, validation_set, n_forecasts, past_covariates_train = None, past_covariates_validation = None, series_list=False, model_name='Model'):\n",
        "    \"\"\"\n",
        "    Fit the model to the training data, make predictions, and evaluate the metrics.\n",
        "\n",
        "    Args:\n",
        "        model: The model to be trained and used for predictions.\n",
        "        training_set: The data to train the model.\n",
        "        validation_set: The data to validate the model.\n",
        "        n_forecasts (int): The number of forecast steps.\n",
        "        past_covariates_train: The past covariates for training.\n",
        "        past_covariates_validation: The past covariates for validation.\n",
        "        series_list (bool, optional): If True, the training_set will be passed as a series to the predict method. Defaults to False.\n",
        "        model_name (str, optional): The name of the model. Defaults to 'Model'.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the forecast and the metrics.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the lengths of actual_values and predicted_values are not the same.\n",
        "    \"\"\"\n",
        "    # Validate inputs\n",
        "    if not model or not training_set or n_forecasts < 1:\n",
        "        raise ValueError(\"Invalid inputs provided.\")\n",
        "\n",
        "    # Fit the model and make predictions\n",
        "    try:\n",
        "        train_start = time.time()\n",
        "        model.fit(training_set, past_covariates=past_covariates_train)\n",
        "        train_stop = time.time()\n",
        "        forecast_start = time.time()\n",
        "        forecast = model.predict(n_forecasts, series=training_set if series_list else None, past_covariates=past_covariates_validation, show_warnings=False)\n",
        "        forecast_stop = time.time()\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while fitting the model or making predictions: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Validate the forecast and actual values length\n",
        "    #if len(validation_set) != len(forecast):\n",
        "     #   raise ValueError(\"Lengths of validation_set and forecast must be the same.\")\n",
        "\n",
        "\n",
        "    mae_value = mae(validation_set, forecast, inter_reduction = np.mean)\n",
        "    smape_value = smape(validation_set, forecast, inter_reduction = np.mean)\n",
        "    rmse_value = rmse(validation_set, forecast, inter_reduction = np.mean)\n",
        "\n",
        "    train_time = train_stop-train_start\n",
        "    forecast_time = forecast_stop - forecast_start\n",
        "\n",
        "    print('-----------------------------------------')\n",
        "    #print(f\"MAE ({model_name}) = {mae_value:.2f}\")\n",
        "    print(f\"sMAPE ({model_name}) = {smape_value:.2f}\")\n",
        "    print(f\"RMSE ({model_name}) = {rmse_value:.2f}\")\n",
        "    print(f\"Training Time ({model_name}) = {train_time:.2f}sec\")\n",
        "    print(f\"Forecast Time ({model_name}) = {forecast_time:.2f}sec\")\n",
        "    print(f\"Total Time ({model_name}) = {train_time+forecast_time:.2f}sec\")\n",
        "    print('-----------------------------------------')\n",
        "\n",
        "    return {'forecast': forecast, 'MAE': mae_value, 'sMAPE': smape_value, 'RMSE': rmse_value, 'Training Time': train_time, 'Forecast Time': forecast_time, 'Total Time': train_time+forecast_time}\n",
        "\n",
        "'----------------------------------------------------------------------------------------------------'\n",
        "\n",
        "\n",
        "def extend_index_with_past(input_chunk_length, train_idx, test_idx, dataframe, gap = False):\n",
        "    \"\"\"\n",
        "    Extends the first index in the list with its previous indexes based on input_chunk_length.\n",
        "\n",
        "    Parameters:\n",
        "    input_chunk_length (int): Number of previous indexes to include.\n",
        "    indexes (list): List of indexes from the DataFrame.\n",
        "    dataframe (DataFrame): The DataFrame from which indexes are derived.\n",
        "\n",
        "    Returns:\n",
        "    list: Extended list of indexes including the range from the first index and its previous indexes.\n",
        "    \"\"\"\n",
        "    if not train_idx or dataframe.empty:\n",
        "        return []\n",
        "\n",
        "    index = max(train_idx)\n",
        "    # Ensuring the start index does not fall below the DataFrame's minimum index\n",
        "    start_index = max(index - input_chunk_length, dataframe.index.min())\n",
        "\n",
        "    extended_indexes = list(range(start_index+1, index + 1))+ test_idx if gap == True else list(range(start_index + 1, max(test_idx) + 1))\n",
        "    return extended_indexes\n",
        "\n",
        "\n",
        "'----------------------------------------------------------------------------------------------------'\n",
        "\n",
        "def add_metrics_to_df(df, store_id, dept_id, model_name, mae, smape, rmse, training_time, forecast_time, total_time, dataset_size):\n",
        "    new_row = pd.DataFrame({\n",
        "        'Store ID': [store_id],\n",
        "        'Dept ID': [dept_id],\n",
        "        'Model Name': [model_name],\n",
        "        'MAE': [mae],\n",
        "        'sMAPE': [smape],\n",
        "        'RMSE': [rmse],\n",
        "        'Training Time(seconds)': [training_time],\n",
        "        'Forecast Time(seconds)': [forecast_time],\n",
        "        'Total Time(seconds)': [total_time],\n",
        "        'Dataset Size(MB)': [dataset_size]\n",
        "    })\n",
        "    df = pd.concat([df, new_row])\n",
        "    return df\n",
        "\n",
        "'-------------------------------------------------------------------------------------------------------'\n",
        "# Function to load datasets\n",
        "def load_dataset(path):\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "'-------------------------------------------------------------------------------------------------------'\n",
        "\n",
        "# Function to convert and one-hot encode events\n",
        "def one_hot_encode_and_join(original_dataframe, feature_to_encode, new_name=None):\n",
        "    dummies = pd.get_dummies(original_dataframe[feature_to_encode].fillna('No_Event'), prefix=new_name)\n",
        "    return original_dataframe.drop(feature_to_encode, axis=1).join(dummies)\n",
        "\n",
        "'-------------------------------------------------------------------------------------------------------'\n",
        "\n",
        "# Creating the DataFrame\n",
        "columns = ['Store ID', 'Dept ID', 'Model Name', 'MAE', 'sMAPE', 'RMSE', 'Training Time(seconds)', 'Forecast Time(seconds)', 'Total Time(seconds)',\n",
        "           'Dataset Size(MB)']\n",
        "metrics_df = pd.DataFrame(columns=columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2cumUb3DWhS"
      },
      "source": [
        "\n",
        "## **Model Creation Functions**\n",
        "\n",
        "In this section, we introduce several functions designed for creating different types of models. These functions are tailored to set up various machine learning models with specific parameters.\n",
        "\n",
        "**LightGBM Model**: <br>\n",
        "This function creates an instance of LightGBMModel with the specified parameters.\n",
        "```python\n",
        "create_lightgbm_model(lags=12, output_chunk_length=1, max_depth=3, lags_past_covariates = None ... , model_name = 'LightGBM')\n",
        "```\n",
        "\n",
        "### **Nbeats Model**\n",
        "This function creates an NBEATSModel with specified parameters. NBEATS is a deep learning model known for its effectiveness in time series forecasting.\n",
        "```python\n",
        "def create_nbeats_model(input_chunk_length=24, output_chunk_length=24, ... , model_name=\"NBEATS\"):\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlU3DPMp0kzY"
      },
      "outputs": [],
      "source": [
        "def create_lightgbm_model(lags=12,\n",
        "                          output_chunk_length=1,\n",
        "                          max_depth=3,\n",
        "                          n_estimators=50,\n",
        "                          learning_rate=0.1,verbose =-1,use_static_covariates = False,num_leaves=31,lags_past_covariates = None, min_data_in_leaf=20,extra_trees=True):\n",
        "    \"\"\"\n",
        "    Creates an instance of LightGBMModel with the specified parameters.\n",
        "\n",
        "    Parameters:\n",
        "    lags (int): Number of lags to use in the model.\n",
        "    output_chunk_length (int): Length of the output chunks.\n",
        "    max_depth (int): Maximum depth of the trees.\n",
        "    n_estimators (int): Number of trees in the model.\n",
        "    learning_rate (float): Learning rate for the model.\n",
        "\n",
        "    Returns:\n",
        "    model (LightGBMModel): An instance of LightGBMModel.\n",
        "    \"\"\"\n",
        "\n",
        "    model = LightGBMModel(\n",
        "        lags=lags,\n",
        "        output_chunk_length=output_chunk_length,\n",
        "        max_depth=max_depth,\n",
        "        n_estimators=n_estimators,\n",
        "        learning_rate=learning_rate,\n",
        "        verbose=verbose,\n",
        "        num_leaves=num_leaves,\n",
        "        min_data_in_leaf=min_data_in_leaf,\n",
        "        extra_trees=extra_trees,\n",
        "        lags_past_covariates = lags_past_covariates\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_nbeats_model(input_chunk_length=24, output_chunk_length=24, generic_architecture=True,\n",
        "               num_stacks=4, num_blocks=2, num_layers=2, layer_widths=16,\n",
        "               n_epochs=2, nr_epochs_val_period=1, batch_size=32, model_name=\"NBEATS\"):\n",
        "    \"\"\"\n",
        "    Create and return an NBEATSModel with the specified parameters.\n",
        "\n",
        "    Args:\n",
        "        input_chunk_length (int): The length of the input sequence.\n",
        "        output_chunk_length (int): The length of the output sequence.\n",
        "        generic_architecture (bool): Whether to use the generic architecture.\n",
        "        num_stacks (int): The number of stacks.\n",
        "        num_blocks (int): The number of blocks per stack.\n",
        "        num_layers (int): The number of layers per block.\n",
        "        layer_widths (int): The number of units per layer.\n",
        "        n_epochs (int): The number of epochs.\n",
        "        nr_epochs_val_period (int): The number of epochs between each validation set evaluation.\n",
        "        batch_size (int): The size of the batch.\n",
        "        model_name (str): The name of the model.\n",
        "\n",
        "    Returns:\n",
        "        NBEATSModel: A trained NBEATSModel.\n",
        "    \"\"\"\n",
        "    model_nbeats = NBEATSModel(\n",
        "        input_chunk_length=input_chunk_length,\n",
        "        output_chunk_length=output_chunk_length,\n",
        "        generic_architecture=generic_architecture,\n",
        "        num_stacks=num_stacks,\n",
        "        num_blocks=num_blocks,\n",
        "        num_layers=num_layers,\n",
        "        layer_widths=layer_widths,\n",
        "        n_epochs=n_epochs,\n",
        "        nr_epochs_val_period=nr_epochs_val_period,\n",
        "        batch_size=batch_size,\n",
        "        model_name=model_name\n",
        "    )\n",
        "\n",
        "    return model_nbeats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn4m6NZYvEW8"
      },
      "source": [
        "## **free_ram_memory function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brjmLlAAtZVI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def reduce_memory(df, verbose=True):\n",
        "    \"\"\"Reduce memory of a DataFrame by downcasting numeric types.\"\"\"\n",
        "\n",
        "    # Initial memory\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "\n",
        "    # Define numeric types and their downcast\n",
        "    type_mapping = {\n",
        "        'int': [(np.int8, np.iinfo(np.int8)),\n",
        "                (np.int16, np.iinfo(np.int16)),\n",
        "                (np.int32, np.iinfo(np.int32)),\n",
        "                (np.int64, np.iinfo(np.int64))],\n",
        "\n",
        "        'float': [(np.float16, np.finfo(np.float16)),\n",
        "                  (np.float32, np.finfo(np.float32)),\n",
        "                  (np.float64, np.finfo(np.float64))]\n",
        "    }\n",
        "\n",
        "    for col, col_type in df.dtypes.items():\n",
        "        if col_type in ['int16', 'int32', 'int64']:\n",
        "            for dtype, type_info in type_mapping['int']:\n",
        "                if type_info.min <= df[col].min() and df[col].max() <= type_info.max:\n",
        "                    df[col] = df[col].astype(dtype)\n",
        "                    break\n",
        "        elif col_type in ['float16', 'float32', 'float64']:\n",
        "            for dtype, type_info in type_mapping['float']:\n",
        "                if type_info.min <= df[col].min() and df[col].max() <= type_info.max:\n",
        "                    df[col] = df[col].astype(dtype)\n",
        "                    break\n",
        "\n",
        "    # Calculate and print memory reduction\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose:\n",
        "        print(f'Memory usage decreased from {start_mem:.2f}MB to {end_mem:.2f}MB ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5lY3n_Pjo4N"
      },
      "source": [
        "# M5: Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data Loading\n",
        "def process_file(file_name, base_path, optimize=False, store=False, pickle = False):\n",
        "    try:\n",
        "        if pickle == False:\n",
        "          print(f\"Reading {file_name}...\")\n",
        "          df = pd.read_csv(base_path + file_name)\n",
        "          print(f\"File Size: {round(df.memory_usage(deep=True).sum() / 1024 ** 2, 2)} MB\")\n",
        "\n",
        "        if optimize:\n",
        "            print(f\"Optimizing {file_name}...\")\n",
        "            df = reduce_memory(df, verbose=True)\n",
        "            print(f\"Size after optimization: {round(df.memory_usage(deep=True).sum() / 1024 ** 2, 2)} MB\\n\")\n",
        "\n",
        "        if store:\n",
        "            df.to_pickle(base_path + file_name.split('.')[0] + '.pkl')\n",
        "            print(f\"Pickle file is stored as {file_name.split('.')[0]}.pkl\")\n",
        "\n",
        "        if pickle:\n",
        "            print(f\"Reading {file_name.split('.')[0]}.pkl...\")\n",
        "            df = pd.read_pickle(base_path + file_name.split('.')[0] + '.pkl')\n",
        "            print(f\"File Size: {round(df.memory_usage(deep=True).sum() / 1024 ** 2, 2)} MB\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    return df\n",
        "\n",
        "# Usage\n",
        "base_path = \"/content/drive/MyDrive/Master_Thesis/Datasets/M5-Forecasting-accuracy/\"\n",
        "#base_path = \"C:/Users/abcr5914/Downloads/M5-Forecasting-accuracy/\"\n",
        "\n",
        "sell_price_df = process_file(\"sell_prices.csv\", base_path, optimize=False, store=False)\n",
        "calendar_df = process_file(\"calendar.csv\", base_path)\n",
        "#sales_train_evaluation_df = process_file(\"sales_train_evaluation.csv\", base_path, optimize=True, store=True)\n",
        "sales_train_evaluation_df = process_file(\"sales_train_evaluation.csv\", base_path, pickle = True)"
      ],
      "metadata": {
        "id": "EPqqbPQZQyGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dates_df = calendar_df[['date','d','wm_yr_wk']].set_index('wm_yr_wk',drop =True)"
      ],
      "metadata": {
        "id": "X74iGFqhQsu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calendar\n",
        "\n",
        "# One-hot encode 'event_name_1' and 'event_name_2'\n",
        "calendar_df = one_hot_encode_and_join(calendar_df, 'event_name_1', 'event')\n",
        "calendar_df = one_hot_encode_and_join(calendar_df, 'event_name_2', 'event2')\n",
        "calendar_df.drop(columns =['event_type_1','event_type_2','month','year','weekday','d'],inplace = True)\n",
        "\n",
        "calendar_df.head(3)"
      ],
      "metadata": {
        "id": "NRmwLLB7Q_Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sales Train Evaluation\n",
        "sales_train = sales_train_evaluation_df.set_index('id').drop(columns = ['item_id', 'dept_id', 'cat_id','store_id', 'state_id'])\n",
        "sales_train = sales_train.T\n",
        "sales_train_date = sales_train.merge(dates_df,left_index=True, right_on='d').set_index('date').drop(columns = ['d']).T\n",
        "sales_train_date.head(3)"
      ],
      "metadata": {
        "id": "daq7BANqRH3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sell Price\n",
        "sell_price_df['item_store_id'] = sell_price_df['item_id'] + '_' + sell_price_df['store_id']+'_evaluation'\n",
        "pivot_df = sell_price_df.pivot(index='item_store_id', columns='wm_yr_wk', values='sell_price').T\n",
        "sell_price_date = pivot_df.merge(dates_df, on='wm_yr_wk', how='inner').set_index('date').drop(columns=['d']).T\n",
        "sell_price_date.head(3)"
      ],
      "metadata": {
        "id": "4MqvOSXIRebK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store_id and dept_id  can be set manually or using generator\n",
        "store_id = 'CA_2'\n",
        "dept_id = 'HOBBIES_1'"
      ],
      "metadata": {
        "id": "-xZ68OSlFHhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Preprocessing Dataset\n",
        "\n",
        "# Sample Dataset\n",
        "sample_train = sales_train_date[sales_train_date.index.str.contains(dept_id) & sales_train_date.index.str.contains(store_id)]\n",
        "sample_sell = sell_price_date[sell_price_date.index.str.contains(dept_id) & sell_price_date.index.str.contains(store_id)]\n",
        "\n",
        "# Melting Dataset\n",
        "melted_train = sample_train.reset_index().melt(id_vars='index', var_name='date', value_name='sales')\n",
        "melted_train = melted_train.rename(columns={'index': 'id'}).sort_values(['id','date']).reset_index(drop=True)\n",
        "\n",
        "melted_sell = sample_sell.loc[:, :'2016-05-22'].reset_index().melt(id_vars='index', var_name='date', value_name='sell_price')\n",
        "melted_sell = melted_sell.rename(columns={'index': 'id'}).sort_values(['id','date']).reset_index(drop=True)\n",
        "\n",
        "# Merging Dataset\n",
        "merged_df = pd.merge(melted_sell, melted_train, on=['date','id'], how='left')\n",
        "merged_df = pd.merge(merged_df, calendar_df, on='date', how='left')\n",
        "merged_df = merged_df.dropna(subset=['sell_price']).reset_index(drop = True)"
      ],
      "metadata": {
        "id": "lm9i8JhedXK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "V0sdTM9wd7Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert to timeseries\n",
        "sorted_data = merged_df.copy().sort_values(by='id')\n",
        "\n",
        "# Initializing the label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoder to the 'id' column and transform it to numeric labels\n",
        "sorted_data['id_encoded'] = label_encoder.fit_transform(sorted_data['id'])\n",
        "\n",
        "# Group by 'id' and convert the 'value' column into TimeSeries objects\n",
        "group_col = ['id_encoded']\n",
        "date_col = 'date'\n",
        "value_col = ['sales']\n",
        "\n",
        "# Generate TimeSeries objects for the target variable\n",
        "grouped_time_series = TimeSeries.from_group_dataframe(\n",
        "    sorted_data,\n",
        "    group_cols=group_col,\n",
        "    time_col=date_col,\n",
        "    value_cols=value_col\n",
        ")\n",
        "\n",
        "# Set the cutoff date for training and validation sets\n",
        "split_date = pd.Timestamp('2016-04-24')\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_set = [ts.split_before(split_date)[0] for ts in grouped_time_series]\n",
        "val_set = [ts.split_after(split_date)[1] for ts in grouped_time_series]\n",
        "\n",
        "#Scale the timeseries\n",
        "Scale_series = Scaler()\n",
        "train_series_set = [Scale_series.fit_transform(series) for series in train_set]\n",
        "val_series_set =[Scale_series.transform(series) for series in val_set]\n",
        "\n",
        "# Gather additional features for the model\n",
        "exogenous_features = ['sell_price','snap_CA', 'snap_TX', 'snap_WI'] + \\\n",
        "    [e for e in sorted_data.columns if e.startswith('event')] + \\\n",
        "    [e for e in sorted_data.columns if e.startswith('event2')]\n",
        "\n",
        "exog_features_data = sorted_data[['id_encoded', 'date'] + exogenous_features].sort_values(by='id_encoded')\n",
        "\n",
        "# Convert the exogenous features into TimeSeries objects\n",
        "exog_features_series = TimeSeries.from_group_dataframe(\n",
        "    exog_features_data,\n",
        "    group_cols=group_col,\n",
        "    time_col=date_col,\n",
        "    value_cols=exogenous_features\n",
        ")\n",
        "\n",
        "# Split date for exogenous features\n",
        "train_exog_cutoff = split_date\n",
        "val_exog_cutoff = pd.Timestamp('2016-03-30')\n",
        "\n",
        "# Split the exogenous features TimeSeries\n",
        "train_exog_set = [ts.split_before(train_exog_cutoff)[0] for ts in exog_features_series]\n",
        "val_exog_set = [ts.split_after(val_exog_cutoff)[1] for ts in exog_features_series]\n",
        "\n",
        "del sorted_data\n",
        "\n",
        "garbage_collected = gc.collect()\n"
      ],
      "metadata": {
        "id": "yG8Z_KlPeaU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hm6zIwFjkke6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "I9R8HMmT54Vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "models = {\n",
        "\n",
        "  # Regression Models\n",
        "  #'Linear Regression Model' : LinearRegressionModel(lags = [-1,-24,-120,-168], lags_past_covariates = 1),\n",
        "\n",
        "  # traditional ML Models\n",
        "  #'Random Forest Model' : RandomForest(lags = [-1,-24,-120,-168], lags_past_covariates=1),\n",
        "  'LightGBM Model': create_lightgbm_model(lags = [-1,-24,-120,-168], lags_past_covariates=3),\n",
        "\n",
        "  # Deep Neural Networks\n",
        "  #'TCN Model' : TCNModel(input_chunk_length=24, output_chunk_length = 12 ,n_epochs=5),\n",
        "  #'NBeats Model' : create_nbeats_model(),\n",
        "\n",
        "  # Transformer\n",
        "  #'TFT Model'  : TFTModel(input_chunk_length = 12, output_chunk_length = 12, full_attention = True)\n",
        "\n",
        "  }\n",
        "\n",
        "results_list = {}\n",
        "metrics_list = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "\n",
        "  print('\\n',name)\n",
        "\n",
        "  results = fit_predict_and_evaluate(\n",
        "    model=model,\n",
        "    training_set=train_series_set,\n",
        "    validation_set=val_series_set,\n",
        "    n_forecasts=30,\n",
        "    past_covariates_train=train_exog_set,\n",
        "    past_covariates_validation=val_exog_set,\n",
        "    series_list=True,\n",
        "    model_name = name\n",
        "    )\n",
        "\n",
        "\n",
        "  results_list[f'{name}_results'] = results\n",
        "  metrics_list[f'{name}_metrics']  = {'SMAPE':results['sMAPE'] ,'Training Time': results['Training Time'],'RMSE': results['RMSE']}"
      ],
      "metadata": {
        "id": "QTJnSjrJ1GtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        " Linear Regression Model\n",
        "-----------------------------------------\n",
        "sMAPE (Linear Regression Model) = 144.59\n",
        "RMSE (Linear Regression Model) = 0.09\n",
        "Training Time (Linear Regression Model) = 6.08sec\n",
        "Forecast Time (Linear Regression Model) = 2.52sec\n",
        "Total Time (Linear Regression Model) = 8.59sec\n",
        "-----------------------------------------\n",
        "\n",
        "-----------------------------------------\n",
        "sMAPE (TCN Model) = 144.99\n",
        "RMSE (TCN Model) = 0.09\n",
        "Training Time (TCN Model) = 3208.28sec\n",
        "Forecast Time (TCN Model) = 1.28sec\n",
        "Total Time (TCN Model) = 3209.56sec\n",
        "-----------------------------------------\n",
        "\n",
        "-----------------------------------------\n",
        "sMAPE (NBeats Model) = 147.18\n",
        "RMSE (NBeats Model) = 0.10\n",
        "Training Time (NBeats Model) = 2339.39sec\n",
        "Forecast Time (NBeats Model) = 1.31sec\n",
        "Total Time (NBeats Model) = 2340.70sec\n",
        "-----------------------------------------\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "U1LtOjWCDqd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_lgbm = create_lightgbm_model(lags_past_covariates = 7, output_chunk_length=2,lags = [-7,-30,-180,-365])\n",
        "\n",
        "\n",
        "results = fit_predict_and_evaluate(\n",
        "    model=model_lgbm,\n",
        "    training_set=train_series_set,\n",
        "    validation_set=val_series_set,\n",
        "    n_forecasts=30,\n",
        "    past_covariates_train=train_exog_set,\n",
        "    past_covariates_validation=val_exog_set,\n",
        "    series_list=True,\n",
        "    model_name='LightGBM'\n",
        ")\n"
      ],
      "metadata": {
        "id": "MkwSJDVGgtMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Optuna**"
      ],
      "metadata": {
        "id": "tE13fdKwpdNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial, training_data, validation_data, model_type, past_cov_train, past_cov_validation,model_name):\n",
        "    \"\"\"\n",
        "    Optuna objective function for hyperparameter optimization.\n",
        "    This function is to find the best parameters for a model\n",
        "    to minimize the Symmetric Mean Absolute Percentage Error (sMAPE)\n",
        "    between predicted and actual values in a validation dataset.\n",
        "    \"\"\"\n",
        "    # Define hyperparameters\n",
        "\n",
        "\n",
        "    batch_size = trial.suggest_categorical('batch_size', [8, 16])\n",
        "\n",
        "    if model_type == 'LightGBM':\n",
        "        output_chunk_length = trial.suggest_int('output_chunk_length', 1,5)\n",
        "        max_depth = trial.suggest_int('max_depth', 2, 10)\n",
        "        n_estimators = trial.suggest_int('n_estimators', 10, 100)\n",
        "        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)\n",
        "        num_leaves = trial.suggest_int('num_leaves', 20, 60)\n",
        "        extra_trees = trial.suggest_categorical('extra_trees', [True, False])\n",
        "\n",
        "        model = create_lightgbm_model(\n",
        "            lags_past_covariates=7,\n",
        "            output_chunk_length=output_chunk_length,\n",
        "            lags=[-7,-30,-180,-365],\n",
        "            max_depth=max_depth,\n",
        "            n_estimators=n_estimators,\n",
        "            learning_rate=learning_rate,\n",
        "            verbose=-1,\n",
        "        )\n",
        "\n",
        "    elif model_type == 'NBEATS':\n",
        "        output_chunk_length = trial.suggest_categorical('output_chunk_length', [12, 24])\n",
        "        n_epochs = trial.suggest_int('n_epochs', 1, 4)\n",
        "        num_stacks = trial.suggest_int('num_stacks', 1, 3)\n",
        "        num_blocks = trial.suggest_int('num_blocks', 2, 6)\n",
        "        num_layers = trial.suggest_int('num_layers', 1, 3)\n",
        "        layer_widths = trial.suggest_categorical('layer_widths', [16, 32])\n",
        "        dropout = trial.suggest_float('dropout', 0.0, 0.2)\n",
        "        activation = trial.suggest_categorical('activation', ['ReLU', 'RReLU', 'LeakyReLU'])\n",
        "        model = create_nbeats_model(\n",
        "            input_chunk_length = 12,\n",
        "            output_chunk_length=output_chunk_length,\n",
        "            generic_architecture=True,\n",
        "            num_stacks=num_stacks,\n",
        "            num_blocks=num_blocks,\n",
        "            num_layers=num_layers,\n",
        "            layer_widths=layer_widths,\n",
        "            n_epochs=n_epochs,\n",
        "            nr_epochs_val_period=1,\n",
        "            batch_size=batch_size,\n",
        "            model_name=\"NBEATS\"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type\")\n",
        "\n",
        "    # Predict and calculate sMAPE\n",
        "    forecast = fit_predict_and_evaluate(\n",
        "        model=model,\n",
        "        training_set=training_data,\n",
        "        validation_set=validation_data,\n",
        "        n_forecasts=28,\n",
        "        past_covariates_train=past_cov_train,\n",
        "        past_covariates_validation=past_cov_validation,\n",
        "        series_list=True,\n",
        "        model_name = model_name\n",
        "    )\n",
        "    smape_result = forecast['sMAPE']\n",
        "    rmse_result = forecast['RMSE']\n",
        "    combined_metric = 0.6 * smape_result + 0.4 * rmse_result\n",
        "\n",
        "    return combined_metric"
      ],
      "metadata": {
        "id": "Mwpo0CMjpgWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study_LGBM = optuna.create_study(direction='minimize')\n",
        "study_LGBM.optimize(lambda trial: objective(trial, training_data = train_series_set, validation_data = val_series_set, past_cov_train= train_exog_set, past_cov_validation = val_exog_set ,model_name = 'LightGBM',model_type = 'LightGBM'), n_trials=10)\n",
        "\n",
        "print('Best Params for LightGBM:')\n",
        "trial_LGBM = study_LGBM.best_trial\n",
        "print(trial_LGBM.params)"
      ],
      "metadata": {
        "id": "lZRxlJHdrTx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plots Optuna\n",
        "display(optuna.visualization.plot_optimization_history(study_LGBM))\n",
        "display(optuna.visualization.plot_param_importances(study_LGBM))\n",
        "display(optuna.visualization.plot_parallel_coordinate(study_LGBM))\n"
      ],
      "metadata": {
        "id": "p5fPZlMu7aZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study_NB = optuna.create_study(direction='minimize')\n",
        "study_NB.optimize(lambda trial: objective(trial, training_data = train_series_set, validation_data = val_series_set, past_cov_train= train_exog_set, past_cov_validation = val_exog_set ,model_name = 'NBEATS',model_type = 'NBEATS'), n_trials=10)\n",
        "\n",
        "print('Best Params for NBEATS:')\n",
        "trial_NB = study_NB.best_trial\n",
        "print(trial_NB.params)"
      ],
      "metadata": {
        "id": "UkkkpXCdtbrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plots Optuna\n",
        "display(optuna.visualization.plot_optimization_history(study_NB))\n",
        "display(optuna.visualization.plot_param_importances(study_NB))\n",
        "display(optuna.visualization.plot_parallel_coordinate(study_NB))\n"
      ],
      "metadata": {
        "id": "oLQMT_Qi1GG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Cross Validation**"
      ],
      "metadata": {
        "id": "ugumS2cksd1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
        "from sklearn.utils.validation import _deprecate_positional_args\n",
        "#(n_splits=(default 5), max_train_size = (default = None))\n",
        "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
        "class GroupTimeSeriesSplit(_BaseKFold):\n",
        "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
        "    Provides train/test indices to split time series data samples\n",
        "    that are observed at fixed time intervals according to a\n",
        "    third-party provided group.\n",
        "    In each split, test indices must be higher than before, and thus shuffling\n",
        "    in cross validator is inappropriate.\n",
        "    This cross-validation object is a variation of :class:`KFold`.\n",
        "    In the kth split, it returns first k folds as train set and the\n",
        "    (k+1)th fold as test set.\n",
        "    The same group will not appear in two different folds (the number of\n",
        "    distinct groups has to be at least equal to the number of folds).\n",
        "    Note that unlike standard cross-validation methods, successive\n",
        "    training sets are supersets of those that come before them.\n",
        "    Read more in the :ref:`User Guide <cross_validation>`.\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_splits : int, default=5\n",
        "        Number of splits. Must be at least 2.\n",
        "    max_train_size : int, default=None\n",
        "        Maximum size for a single training set.\n",
        "    Examples\n",
        "    --------\n",
        "    >>> import numpy as np\n",
        "    >>> from sklearn.model_selection import GroupTimeSeriesSplit\n",
        "    >>> groups = np.array(['a', 'a', 'a', 'a', 'a', 'a',\\\n",
        "                           'b', 'b', 'b', 'b', 'b',\\\n",
        "                           'c', 'c', 'c', 'c',\\\n",
        "                           'd', 'd', 'd'])\n",
        "    >>> gtss = GroupTimeSeriesSplit(n_splits=3)\n",
        "    >>> for train_idx, test_idx in gtss.split(groups, groups=groups):\n",
        "    ...     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
        "    ...     print(\"TRAIN GROUP:\", groups[train_idx],\\\n",
        "                  \"TEST GROUP:\", groups[test_idx])\n",
        "    TRAIN: [0, 1, 2, 3, 4, 5] TEST: [6, 7, 8, 9, 10]\n",
        "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a']\\\n",
        "    TEST GROUP: ['b' 'b' 'b' 'b' 'b']\n",
        "    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] TEST: [11, 12, 13, 14]\n",
        "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b']\\\n",
        "    TEST GROUP: ['c' 'c' 'c' 'c']\n",
        "    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\\\n",
        "    TEST: [15, 16, 17]\n",
        "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b' 'c' 'c' 'c' 'c']\\\n",
        "    TEST GROUP: ['d' 'd' 'd']\n",
        "    \"\"\"\n",
        "    @_deprecate_positional_args\n",
        "    def __init__(self,\n",
        "                 n_splits=5,\n",
        "                 *,\n",
        "                 max_train_size=None\n",
        "                 ):\n",
        "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
        "        self.max_train_size = max_train_size\n",
        "\n",
        "    def split(self, X, y=None, groups=None):\n",
        "        \"\"\"Generate indices to split data into training and test set.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training data, where n_samples is the number of samples\n",
        "            and n_features is the number of features.\n",
        "        y : array-like of shape (n_samples,)\n",
        "            Always ignored, exists for compatibility.\n",
        "        groups : array-like of shape (n_samples,)\n",
        "            Group labels for the samples used while splitting the dataset into\n",
        "            train/test set.\n",
        "        Yields\n",
        "        ------\n",
        "        train : ndarray\n",
        "            The training set indices for that split.\n",
        "        test : ndarray\n",
        "            The testing set indices for that split.\n",
        "        \"\"\"\n",
        "        if groups is None:\n",
        "            raise ValueError(\n",
        "                \"The 'groups' parameter should not be None\")\n",
        "        X, y, groups = indexable(X, y, groups)\n",
        "        n_samples = _num_samples(X)\n",
        "        n_splits = self.n_splits\n",
        "        n_folds = n_splits + 1\n",
        "        group_dict = {}\n",
        "        u, ind = np.unique(groups, return_index=True)\n",
        "        unique_groups = u[np.argsort(ind)]\n",
        "        n_samples = _num_samples(X)\n",
        "        n_groups = _num_samples(unique_groups)\n",
        "        for idx in np.arange(n_samples):\n",
        "            if (groups[idx] in group_dict):\n",
        "                group_dict[groups[idx]].append(idx)\n",
        "            else:\n",
        "                group_dict[groups[idx]] = [idx]\n",
        "        if n_folds > n_groups:\n",
        "            raise ValueError(\n",
        "                (\"Cannot have number of folds={0} greater than\"\n",
        "                 \" the number of groups={1}\").format(n_folds,\n",
        "                                                     n_groups))\n",
        "        group_test_size = n_groups // n_folds\n",
        "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
        "                                  n_groups, group_test_size)\n",
        "        for group_test_start in group_test_starts:\n",
        "            train_array = []\n",
        "            test_array = []\n",
        "            for train_group_idx in unique_groups[:group_test_start]:\n",
        "                train_array_tmp = group_dict[train_group_idx]\n",
        "                train_array = np.sort(np.unique(\n",
        "                                      np.concatenate((train_array,\n",
        "                                                      train_array_tmp)),\n",
        "                                      axis=None), axis=None)\n",
        "            train_end = train_array.size\n",
        "            if self.max_train_size and self.max_train_size < train_end:\n",
        "                train_array = train_array[train_end -\n",
        "                                          self.max_train_size:train_end]\n",
        "            for test_group_idx in unique_groups[group_test_start:\n",
        "                                                group_test_start +\n",
        "                                                group_test_size]:\n",
        "                test_array_tmp = group_dict[test_group_idx]\n",
        "                test_array = np.sort(np.unique(\n",
        "                                              np.concatenate((test_array,\n",
        "                                                              test_array_tmp)),\n",
        "                                     axis=None), axis=None)\n",
        "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
        "\n",
        "\n",
        "\n",
        "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
        "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
        "    Allows for a gap in groups to avoid potentially leaking info from\n",
        "    train into test if the model has windowed or lag features.\n",
        "    Provides train/test indices to split time series data samples\n",
        "    that are observed at fixed time intervals according to a\n",
        "    third-party provided group.\n",
        "    In each split, test indices must be higher than before, and thus shuffling\n",
        "    in cross validator is inappropriate.\n",
        "    This cross-validation object is a variation of :class:`KFold`.\n",
        "    In the kth split, it returns first k folds as train set and the\n",
        "    (k+1)th fold as test set.\n",
        "    The same group will not appear in two different folds (the number of\n",
        "    distinct groups has to be at least equal to the number of folds).\n",
        "    Note that unlike standard cross-validation methods, successive\n",
        "    training sets are supersets of those that come before them.\n",
        "    Read more in the :ref:`User Guide <cross_validation>`.\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_splits : int, default=5\n",
        "        Number of splits. Must be at least 2.\n",
        "    max_train_group_size : int, default=Inf\n",
        "        Maximum group size for a single training set.\n",
        "    group_gap : int, default=None\n",
        "        Gap between train and test\n",
        "    max_test_group_size : int, default=Inf\n",
        "        We discard this number of groups from the end of each train split\n",
        "    \"\"\"\n",
        "\n",
        "    @_deprecate_positional_args\n",
        "    def __init__(self,\n",
        "                 n_splits=5,\n",
        "                 *,\n",
        "                 max_train_group_size=np.inf,\n",
        "                 max_test_group_size=np.inf,\n",
        "                 group_gap=None,\n",
        "                 verbose=False\n",
        "                 ):\n",
        "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
        "        self.max_train_group_size = max_train_group_size\n",
        "        self.group_gap = group_gap\n",
        "        self.max_test_group_size = max_test_group_size\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def split(self, X, y=None, groups=None):\n",
        "        \"\"\"Generate indices to split data into training and test set.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training data, where n_samples is the number of samples\n",
        "            and n_features is the number of features.\n",
        "        y : array-like of shape (n_samples,)\n",
        "            Always ignored, exists for compatibility.\n",
        "        groups : array-like of shape (n_samples,)\n",
        "            Group labels for the samples used while splitting the dataset into\n",
        "            train/test set.\n",
        "        Yields\n",
        "        ------\n",
        "        train : ndarray\n",
        "            The training set indices for that split.\n",
        "        test : ndarray\n",
        "            The testing set indices for that split.\n",
        "        \"\"\"\n",
        "        if groups is None:\n",
        "            raise ValueError(\n",
        "                \"The 'groups' parameter should not be None\")\n",
        "        X, y, groups = indexable(X, y, groups)\n",
        "        n_samples = _num_samples(X)\n",
        "        n_splits = self.n_splits\n",
        "        group_gap = self.group_gap\n",
        "        max_test_group_size = self.max_test_group_size\n",
        "        max_train_group_size = self.max_train_group_size\n",
        "        n_folds = n_splits + 1\n",
        "        group_dict = {}\n",
        "        u, ind = np.unique(groups, return_index=True)\n",
        "        unique_groups = u[np.argsort(ind)]\n",
        "        n_samples = _num_samples(X)\n",
        "        n_groups = _num_samples(unique_groups)\n",
        "        for idx in np.arange(n_samples):\n",
        "            if (groups[idx] in group_dict):\n",
        "                group_dict[groups[idx]].append(idx)\n",
        "            else:\n",
        "                group_dict[groups[idx]] = [idx]\n",
        "        if n_folds > n_groups:\n",
        "            raise ValueError(\n",
        "                (\"Cannot have number of folds={0} greater than\"\n",
        "                 \" the number of groups={1}\").format(n_folds,\n",
        "                                                     n_groups))\n",
        "\n",
        "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
        "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
        "                                  n_groups, group_test_size)\n",
        "        for group_test_start in group_test_starts:\n",
        "            train_array = []\n",
        "            test_array = []\n",
        "\n",
        "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
        "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
        "                train_array_tmp = group_dict[train_group_idx]\n",
        "\n",
        "                train_array = np.sort(np.unique(\n",
        "                                      np.concatenate((train_array,\n",
        "                                                      train_array_tmp)),\n",
        "                                      axis=None), axis=None)\n",
        "\n",
        "            train_end = train_array.size\n",
        "\n",
        "            for test_group_idx in unique_groups[group_test_start:\n",
        "                                                group_test_start +\n",
        "                                                group_test_size]:\n",
        "                test_array_tmp = group_dict[test_group_idx]\n",
        "                test_array = np.sort(np.unique(\n",
        "                                              np.concatenate((test_array,\n",
        "                                                              test_array_tmp)),\n",
        "                                     axis=None), axis=None)\n",
        "\n",
        "            test_array  = test_array[group_gap:]\n",
        "\n",
        "\n",
        "            if self.verbose > 0:\n",
        "                    pass\n",
        "\n",
        "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
        "\n"
      ],
      "metadata": {
        "id": "DhKysJyUt5I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cv = merged_df.copy()\n",
        "\n",
        "# Convert 'date' column to datetime\n",
        "df_cv['date'] = pd.to_datetime(df_cv['date'])\n",
        "\n",
        "# Create sub-group identifiers combining 'id' and the month\n",
        "df_cv['sub_group'] = df_cv['id'] + '_' + df_cv['date'].dt.to_period('M').astype(str)\n",
        "\n",
        "X = df_cv[['id', 'date', 'sales']]  # Features including 'id' for grouping\n",
        "groups = df_cv['sub_group']         # Sub-group as the group labels\n"
      ],
      "metadata": {
        "id": "FRWxYNt9t_CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Group Cross Validation**"
      ],
      "metadata": {
        "id": "yRxGb3oll7Rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_splits = []\n",
        "test_data_splits = []\n",
        "train_exog_splits = []\n",
        "test_exog_splits = []\n",
        "\n",
        "n_splits = 5\n",
        "\n",
        "exogenous_features = ['sell_price', 'snap_CA', 'snap_TX', 'snap_WI'] + \\\n",
        "    [e for e in df_cv.columns if e.startswith('event')] + \\\n",
        "    [e for e in df_cv.columns if e.startswith('event2')]\n",
        "\n",
        "\n",
        "# Performing cross-validation for each group with sub-groups\n",
        "gtss = GroupTimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "unique_groups = df_cv['id'].unique()\n",
        "for group in unique_groups:\n",
        "\n",
        "    group_data = df_cv[df_cv['id'] == group].reset_index(drop=True)\n",
        "\n",
        "    X_group = group_data  # Group-specific feature\n",
        "    sub_groups = group_data['sub_group']     # Sub-groups\n",
        "\n",
        "    for train_idx, test_idx in gtss.split(X_group, groups=sub_groups):\n",
        "\n",
        "        test_exog_idx = extend_index_with_past(24,train_idx, test_idx, X_group)\n",
        "\n",
        "        X_train, X_test, X_test_exog = X_group.iloc[train_idx], X_group.iloc[test_idx],X_group.iloc[test_exog_idx]\n",
        "\n",
        "        X_train_ts, X_test_ts = TimeSeries.from_dataframe(X_train, time_col='date', value_cols='sales'),TimeSeries.from_dataframe(X_test, time_col='date', value_cols='sales')\n",
        "        exog_train_ts, exog_test_ts = TimeSeries.from_dataframe(X_train, time_col='date', value_cols=exogenous_features),TimeSeries.from_dataframe(X_test_exog, time_col='date', value_cols=exogenous_features)\n",
        "\n",
        "        train_data_splits.append(X_train_ts)\n",
        "        test_data_splits.append(X_test_ts)\n",
        "        train_exog_splits.append(exog_train_ts)\n",
        "        test_exog_splits.append(exog_test_ts)\n",
        "\n",
        "# Splitting the train and test data splits into lists, one for each split\n",
        "train_splits = [train_data_splits[i::n_splits] for i in range(n_splits)]\n",
        "test_splits = [test_data_splits[i::n_splits] for i in range(n_splits)]\n",
        "exog_train_splits = [train_exog_splits[i::n_splits] for i in range(n_splits)]\n",
        "exog_test_splits = [test_exog_splits[i::n_splits] for i in range(n_splits)]\n",
        "\n",
        "del train_data_splits, test_data_splits, train_exog_splits, test_exog_splits\n",
        "\n"
      ],
      "metadata": {
        "id": "JQajcDQQzFue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LightGBM\n",
        "\n",
        "model_lgbm = create_lightgbm_model(lags_past_covariates = 2)\n",
        "\n",
        "for train_split, test_split, exog_train, exog_test in zip(train_splits, test_splits, exog_train_splits, exog_test_splits ):\n",
        "\n",
        "  results_gcv_lgbm = fit_predict_and_evaluate(\n",
        "      model=model_lgbm,\n",
        "      training_set=train_split,\n",
        "      validation_set=test_split,\n",
        "      n_forecasts=28,\n",
        "      past_covariates_train=exog_train,\n",
        "      past_covariates_validation=exog_test,\n",
        "      series_list=True,\n",
        "      model_name='LightGBM'\n",
        "  )"
      ],
      "metadata": {
        "id": "N32MCAlYAmrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nbeats\n",
        "\n",
        "model_nb_gcv = create_nbeats_model()\n",
        "\n",
        "for train_split, test_split, exog_train, exog_test in zip(train_splits, test_splits, exog_train_splits, exog_test_splits ):\n",
        "\n",
        "  results = fit_predict_and_evaluate(\n",
        "      model=model_nb_gcv,\n",
        "      training_set=train_split,\n",
        "      validation_set=test_split,\n",
        "      n_forecasts=28,\n",
        "      past_covariates_train=exog_train,\n",
        "      past_covariates_validation=exog_test,\n",
        "      series_list=True,\n",
        "      model_name='NBeats'\n",
        "  )"
      ],
      "metadata": {
        "id": "0VoLXpNGAo4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Purged group cross validation**"
      ],
      "metadata": {
        "id": "MiEvXGYqlxRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_splits = []\n",
        "test_data_splits = []\n",
        "train_exog_splits = []\n",
        "test_exog_splits = []\n",
        "\n",
        "n_splits = 5\n",
        "\n",
        "exogenous_features = ['sell_price', 'snap_CA', 'snap_TX', 'snap_WI'] + \\\n",
        "    [e for e in df_cv.columns if e.startswith('event')] + \\\n",
        "    [e for e in df_cv.columns if e.startswith('event2')]\n",
        "\n",
        "\n",
        "# Performing cross-validation for each group with sub-groups\n",
        "pgtss = PurgedGroupTimeSeriesSplit(n_splits=n_splits, group_gap=1,max_train_group_size=15,max_test_group_size=5)\n",
        "\n",
        "unique_groups = df_cv['id'].unique()\n",
        "for group in unique_groups:\n",
        "\n",
        "    group_data = df_cv[df_cv['id'] == group].reset_index(drop=True)\n",
        "\n",
        "    X_group = group_data  # Group-specific feature\n",
        "    sub_groups = group_data['sub_group']     # Sub-groups\n",
        "\n",
        "    for train_idx, test_idx in pgtss.split(X_group, groups=sub_groups):\n",
        "\n",
        "        test_exog_idx = extend_index_with_past(24, train_idx, test_idx, X_group)\n",
        "\n",
        "        X_train, X_test, X_test_exog = X_group.iloc[train_idx], X_group.iloc[test_idx],X_group.iloc[test_exog_idx]\n",
        "\n",
        "        X_train_ts, X_test_ts = TimeSeries.from_dataframe(X_train, time_col='date', value_cols='sales'),TimeSeries.from_dataframe(X_test, time_col='date', value_cols='sales')\n",
        "        exog_train_ts, exog_test_ts = TimeSeries.from_dataframe(X_train, time_col='date', value_cols=exogenous_features),TimeSeries.from_dataframe(X_test_exog, time_col='date', value_cols=exogenous_features)\n",
        "\n",
        "        train_data_splits.append(X_train_ts)\n",
        "        test_data_splits.append(X_test_ts)\n",
        "        train_exog_splits.append(exog_train_ts)\n",
        "        test_exog_splits.append(exog_test_ts)\n",
        "\n",
        "# Splitting the train and test data splits into lists, one for each split\n",
        "train_splits = [train_data_splits[i::n_splits] for i in range(n_splits)]\n",
        "test_splits = [test_data_splits[i::n_splits] for i in range(n_splits)]\n",
        "exog_train_splits = [train_exog_splits[i::n_splits] for i in range(n_splits)]\n",
        "exog_test_splits = [test_exog_splits[i::n_splits] for i in range(n_splits)]\n",
        "\n",
        "del train_data_splits, test_data_splits,  train_exog_splits, test_exog_splits\n",
        "\n"
      ],
      "metadata": {
        "id": "b3TJq3-nd_9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LightGBM\n",
        "\n",
        "model_lgbm = create_lightgbm_model(lags_past_covariates = 2)\n",
        "\n",
        "for train_split, test_split, exog_train, exog_test in zip(train_splits, test_splits, exog_train_splits, exog_test_splits ):\n",
        "\n",
        "  results = fit_predict_and_evaluate(\n",
        "      model=model_lgbm,\n",
        "      training_set=train_split,\n",
        "      validation_set=test_split,\n",
        "      n_forecasts=35,\n",
        "      past_covariates_train=exog_train,\n",
        "      past_covariates_validation=exog_test,\n",
        "      series_list=True,\n",
        "      model_name='LightGBM'\n",
        "  )"
      ],
      "metadata": {
        "id": "kSyuZpbDEYBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nbeats\n",
        "\n",
        "model_nb_pgcv = create_nbeats_model()\n",
        "\n",
        "for train_split, test_split, exog_train, exog_test in zip(train_splits, test_splits, exog_train_splits, exog_test_splits ):\n",
        "\n",
        "  results = fit_predict_and_evaluate(\n",
        "      model=model_nb_pgcv,\n",
        "      training_set=train_split,\n",
        "      validation_set=test_split,\n",
        "      n_forecasts=35,\n",
        "      past_covariates_train=exog_train,\n",
        "      past_covariates_validation=exog_test,\n",
        "      series_list=True,\n",
        "      model_name='Nbeats'\n",
        "  )"
      ],
      "metadata": {
        "id": "W7LIjkNNEY81"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}